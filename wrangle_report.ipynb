{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data warangling report\n",
    "\n",
    "The first step of data wrangling is gathering of the datasets. There are many ways of **gathering data**, the one I used for this project are\n",
    "> 1. dowloading data manually.\n",
    "some data are readly available for downloading. Twitter Archive dataset was downloaded by click on it's link and download was initiated.\n",
    "> 2. Downloaodng file programatically using python requests library.\n",
    "we use request to download the image image prediction data. Dowloading data this way is sai to be scalable and reproducable.\n",
    "\n",
    "> 3. Access data through APIs.\n",
    "APIs allows simple access to data easily. For eaxmple in this project we used twitter api to access additional data for our twitter archive dataset, we just had to used tweets_ids's to get the data we required for the current information we have on each dog.\n",
    "\n",
    "### ASSESSING DATASETS.\n",
    "This is the second step in data wrangling process. Data can be assessed both vissualy and programmatically. We asses data to check for data quality and tidiness isssues. When assessing data, I used the **Data Quality Dimensions** to guide myself to find the data issues.\n",
    "From this step I noticed the following data issues in the datasets above:\n",
    "#### Quality Issues:\n",
    "#### Using Visual Assessment.\n",
    "> Thereuse was use of *None* to represent missing data instead of null value i.e np.nan, that can recognised by data tools as null values.\n",
    "> Some names looked incomplete. I.e 'a' as name looked invalid to me.\n",
    "> A column had html tags in it.\n",
    "#### Using Programatic Assessment.\n",
    "> Some columns were represented in wrong data type formats. i.e timestamp was in string instead of datetiem  datatype.\n",
    "> The were some missing data in the Archive twitter data sets.\n",
    "\n",
    "#### Tidiness Issues.\n",
    "> I noticed in twitter Archive data that the dogs stages were observations and it violated the \"Each observation forms a row\"\n",
    "> The three datasets were a single observation unit and it had to be put in one table.\n",
    "\n",
    "### Data Cleaning.\n",
    "> In this 3rd-step of data wrangling I **clean all issues** I documented on the Assessment step. Before I started data cleaning I made copies of the data to avoid affecting the original datasets. I used **define-code-test framework** in this step to clean the datasets. I cleaned data to merged all the required required rules of tidy data.\n",
    "\n",
    "### Storing Data.\n",
    "> After data cleaning step was complete I stored the data in a\n",
    "new csv-file. I named the new csv file as\n",
    "\"twitter_archive_master.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
